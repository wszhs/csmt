{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a827eaa",
   "metadata": {},
   "source": [
    "This exmaple will show you how to interpret anomalies in univariate *time-series* samples.\n",
    "\n",
    "Here we use a HDFS log anomaly detector, named *Deeplog* (CCS'17).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ea87f",
   "metadata": {},
   "source": [
    "# Prepare the Deeplog model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f98d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46337993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], train_loss: 3.0241\n",
      "Epoch [2/300], train_loss: 1.9041\n",
      "Epoch [3/300], train_loss: 1.7699\n",
      "Epoch [4/300], train_loss: 1.6208\n",
      "Epoch [5/300], train_loss: 1.3700\n",
      "Epoch [6/300], train_loss: 1.0587\n",
      "Epoch [7/300], train_loss: 0.8723\n",
      "Epoch [8/300], train_loss: 0.7920\n",
      "Epoch [9/300], train_loss: 0.7411\n",
      "Epoch [10/300], train_loss: 0.6915\n",
      "Epoch [11/300], train_loss: 0.6429\n",
      "Epoch [12/300], train_loss: 0.5949\n",
      "Epoch [13/300], train_loss: 0.5595\n",
      "Epoch [14/300], train_loss: 0.5324\n",
      "Epoch [15/300], train_loss: 0.5076\n",
      "Epoch [16/300], train_loss: 0.4880\n",
      "Epoch [17/300], train_loss: 0.4668\n",
      "Epoch [18/300], train_loss: 0.4475\n",
      "Epoch [19/300], train_loss: 0.4332\n",
      "Epoch [20/300], train_loss: 0.4234\n",
      "Epoch [21/300], train_loss: 0.4153\n",
      "Epoch [22/300], train_loss: 0.4066\n",
      "Epoch [23/300], train_loss: 0.3986\n",
      "Epoch [24/300], train_loss: 0.3950\n",
      "Epoch [25/300], train_loss: 0.3886\n",
      "Epoch [26/300], train_loss: 0.3845\n",
      "Epoch [27/300], train_loss: 0.3797\n",
      "Epoch [28/300], train_loss: 0.3767\n",
      "Epoch [29/300], train_loss: 0.3721\n",
      "Epoch [30/300], train_loss: 0.3679\n",
      "Epoch [31/300], train_loss: 0.3624\n",
      "Epoch [32/300], train_loss: 0.3576\n",
      "Epoch [33/300], train_loss: 0.3506\n",
      "Epoch [34/300], train_loss: 0.3448\n",
      "Epoch [35/300], train_loss: 0.3383\n",
      "Epoch [36/300], train_loss: 0.3332\n",
      "Epoch [37/300], train_loss: 0.3297\n",
      "Epoch [38/300], train_loss: 0.3244\n",
      "Epoch [39/300], train_loss: 0.3193\n",
      "Epoch [40/300], train_loss: 0.3148\n",
      "Epoch [41/300], train_loss: 0.3108\n",
      "Epoch [42/300], train_loss: 0.3069\n",
      "Epoch [43/300], train_loss: 0.3007\n",
      "Epoch [44/300], train_loss: 0.2974\n",
      "Epoch [45/300], train_loss: 0.2932\n",
      "Epoch [46/300], train_loss: 0.2897\n",
      "Epoch [47/300], train_loss: 0.2863\n",
      "Epoch [48/300], train_loss: 0.2831\n",
      "Epoch [49/300], train_loss: 0.2809\n",
      "Epoch [50/300], train_loss: 0.2765\n",
      "Epoch [51/300], train_loss: 0.2732\n",
      "Epoch [52/300], train_loss: 0.2700\n",
      "Epoch [53/300], train_loss: 0.2666\n",
      "Epoch [54/300], train_loss: 0.2662\n",
      "Epoch [55/300], train_loss: 0.2626\n",
      "Epoch [56/300], train_loss: 0.2608\n",
      "Epoch [57/300], train_loss: 0.2588\n",
      "Epoch [58/300], train_loss: 0.2572\n",
      "Epoch [59/300], train_loss: 0.2534\n",
      "Epoch [60/300], train_loss: 0.2513\n",
      "Epoch [61/300], train_loss: 0.2493\n",
      "Epoch [62/300], train_loss: 0.2493\n",
      "Epoch [63/300], train_loss: 0.2484\n",
      "Epoch [64/300], train_loss: 0.2466\n",
      "Epoch [65/300], train_loss: 0.2442\n",
      "Epoch [66/300], train_loss: 0.2420\n",
      "Epoch [67/300], train_loss: 0.2416\n",
      "Epoch [68/300], train_loss: 0.2398\n",
      "Epoch [69/300], train_loss: 0.2381\n",
      "Epoch [70/300], train_loss: 0.2365\n",
      "Epoch [71/300], train_loss: 0.2364\n",
      "Epoch [72/300], train_loss: 0.2352\n",
      "Epoch [73/300], train_loss: 0.2344\n",
      "Epoch [74/300], train_loss: 0.2321\n",
      "Epoch [75/300], train_loss: 0.2324\n",
      "Epoch [76/300], train_loss: 0.2308\n",
      "Epoch [77/300], train_loss: 0.2290\n",
      "Epoch [78/300], train_loss: 0.2306\n",
      "Epoch [79/300], train_loss: 0.2283\n",
      "Epoch [80/300], train_loss: 0.2279\n",
      "Epoch [81/300], train_loss: 0.2264\n",
      "Epoch [82/300], train_loss: 0.2264\n",
      "Epoch [83/300], train_loss: 0.2262\n",
      "Epoch [84/300], train_loss: 0.2238\n",
      "Epoch [85/300], train_loss: 0.2232\n",
      "Epoch [86/300], train_loss: 0.2222\n",
      "Epoch [87/300], train_loss: 0.2222\n",
      "Epoch [88/300], train_loss: 0.2226\n",
      "Epoch [89/300], train_loss: 0.2203\n",
      "Epoch [90/300], train_loss: 0.2198\n",
      "Epoch [91/300], train_loss: 0.2196\n",
      "Epoch [92/300], train_loss: 0.2191\n",
      "Epoch [93/300], train_loss: 0.2186\n",
      "Epoch [94/300], train_loss: 0.2181\n",
      "Epoch [95/300], train_loss: 0.2175\n",
      "Epoch [96/300], train_loss: 0.2170\n",
      "Epoch [97/300], train_loss: 0.2161\n",
      "Epoch [98/300], train_loss: 0.2156\n",
      "Epoch [99/300], train_loss: 0.2148\n",
      "Epoch [100/300], train_loss: 0.2142\n",
      "Epoch [101/300], train_loss: 0.2142\n",
      "Epoch [102/300], train_loss: 0.2140\n",
      "Epoch [103/300], train_loss: 0.2132\n",
      "Epoch [104/300], train_loss: 0.2141\n",
      "Epoch [105/300], train_loss: 0.2132\n",
      "Epoch [106/300], train_loss: 0.2134\n",
      "Epoch [107/300], train_loss: 0.2130\n",
      "Epoch [108/300], train_loss: 0.2121\n",
      "Epoch [109/300], train_loss: 0.2116\n",
      "Epoch [110/300], train_loss: 0.2111\n",
      "Epoch [111/300], train_loss: 0.2108\n",
      "Epoch [112/300], train_loss: 0.2109\n",
      "Epoch [113/300], train_loss: 0.2108\n",
      "Epoch [114/300], train_loss: 0.2110\n",
      "Epoch [115/300], train_loss: 0.2109\n",
      "Epoch [116/300], train_loss: 0.2098\n",
      "Epoch [117/300], train_loss: 0.2103\n",
      "Epoch [118/300], train_loss: 0.2083\n",
      "Epoch [119/300], train_loss: 0.2080\n",
      "Epoch [120/300], train_loss: 0.2083\n",
      "Epoch [121/300], train_loss: 0.2085\n",
      "Epoch [122/300], train_loss: 0.2084\n",
      "Epoch [123/300], train_loss: 0.2085\n",
      "Epoch [124/300], train_loss: 0.2078\n",
      "Epoch [125/300], train_loss: 0.2074\n",
      "Epoch [126/300], train_loss: 0.2073\n",
      "Epoch [127/300], train_loss: 0.2063\n",
      "Epoch [128/300], train_loss: 0.2061\n",
      "Epoch [129/300], train_loss: 0.2056\n",
      "Epoch [130/300], train_loss: 0.2074\n",
      "Epoch [131/300], train_loss: 0.2070\n",
      "Epoch [132/300], train_loss: 0.2057\n",
      "Epoch [133/300], train_loss: 0.2051\n",
      "Epoch [134/300], train_loss: 0.2058\n",
      "Epoch [135/300], train_loss: 0.2045\n",
      "Epoch [136/300], train_loss: 0.2052\n",
      "Epoch [137/300], train_loss: 0.2055\n",
      "Epoch [138/300], train_loss: 0.2049\n",
      "Epoch [139/300], train_loss: 0.2045\n",
      "Epoch [140/300], train_loss: 0.2044\n",
      "Epoch [141/300], train_loss: 0.2033\n",
      "Epoch [142/300], train_loss: 0.2034\n",
      "Epoch [143/300], train_loss: 0.2040\n",
      "Epoch [144/300], train_loss: 0.2039\n",
      "Epoch [145/300], train_loss: 0.2033\n",
      "Epoch [146/300], train_loss: 0.2032\n",
      "Epoch [147/300], train_loss: 0.2030\n",
      "Epoch [148/300], train_loss: 0.2039\n",
      "Epoch [149/300], train_loss: 0.2026\n",
      "Epoch [150/300], train_loss: 0.2023\n",
      "Epoch [151/300], train_loss: 0.2029\n",
      "Epoch [152/300], train_loss: 0.2033\n",
      "Epoch [153/300], train_loss: 0.2028\n",
      "Epoch [154/300], train_loss: 0.2022\n",
      "Epoch [155/300], train_loss: 0.2016\n",
      "Epoch [156/300], train_loss: 0.2020\n",
      "Epoch [157/300], train_loss: 0.2016\n",
      "Epoch [158/300], train_loss: 0.2009\n",
      "Epoch [159/300], train_loss: 0.2016\n",
      "Epoch [160/300], train_loss: 0.2011\n",
      "Epoch [161/300], train_loss: 0.2010\n",
      "Epoch [162/300], train_loss: 0.2023\n",
      "Epoch [163/300], train_loss: 0.2001\n",
      "Epoch [164/300], train_loss: 0.2011\n",
      "Epoch [165/300], train_loss: 0.2005\n",
      "Epoch [166/300], train_loss: 0.2001\n",
      "Epoch [167/300], train_loss: 0.1996\n",
      "Epoch [168/300], train_loss: 0.2007\n",
      "Epoch [169/300], train_loss: 0.1996\n",
      "Epoch [170/300], train_loss: 0.2001\n",
      "Epoch [171/300], train_loss: 0.1990\n",
      "Epoch [172/300], train_loss: 0.2000\n",
      "Epoch [173/300], train_loss: 0.1995\n",
      "Epoch [174/300], train_loss: 0.2002\n",
      "Epoch [175/300], train_loss: 0.1990\n",
      "Epoch [176/300], train_loss: 0.1999\n",
      "Epoch [177/300], train_loss: 0.1989\n",
      "Epoch [178/300], train_loss: 0.1995\n",
      "Epoch [179/300], train_loss: 0.1987\n",
      "Epoch [180/300], train_loss: 0.1989\n",
      "Epoch [181/300], train_loss: 0.1989\n",
      "Epoch [182/300], train_loss: 0.1997\n",
      "Epoch [183/300], train_loss: 0.1994\n",
      "Epoch [184/300], train_loss: 0.1983\n",
      "Epoch [185/300], train_loss: 0.1981\n",
      "Epoch [186/300], train_loss: 0.1978\n",
      "Epoch [187/300], train_loss: 0.1979\n",
      "Epoch [188/300], train_loss: 0.1978\n",
      "Epoch [189/300], train_loss: 0.1983\n",
      "Epoch [190/300], train_loss: 0.1982\n",
      "Epoch [191/300], train_loss: 0.1977\n",
      "Epoch [192/300], train_loss: 0.1977\n",
      "Epoch [193/300], train_loss: 0.1975\n",
      "Epoch [194/300], train_loss: 0.1969\n",
      "Epoch [195/300], train_loss: 0.1973\n",
      "Epoch [196/300], train_loss: 0.1975\n",
      "Epoch [197/300], train_loss: 0.1981\n",
      "Epoch [198/300], train_loss: 0.1980\n",
      "Epoch [199/300], train_loss: 0.1978\n",
      "Epoch [200/300], train_loss: 0.1973\n",
      "Epoch [201/300], train_loss: 0.1969\n",
      "Epoch [202/300], train_loss: 0.1968\n",
      "Epoch [203/300], train_loss: 0.1966\n",
      "Epoch [204/300], train_loss: 0.1976\n",
      "Epoch [205/300], train_loss: 0.1961\n",
      "Epoch [206/300], train_loss: 0.1962\n",
      "Epoch [207/300], train_loss: 0.1954\n",
      "Epoch [208/300], train_loss: 0.1955\n",
      "Epoch [209/300], train_loss: 0.1957\n",
      "Epoch [210/300], train_loss: 0.1962\n",
      "Epoch [211/300], train_loss: 0.1971\n",
      "Epoch [212/300], train_loss: 0.1954\n",
      "Epoch [213/300], train_loss: 0.1953\n",
      "Epoch [214/300], train_loss: 0.1959\n",
      "Epoch [215/300], train_loss: 0.1951\n",
      "Epoch [216/300], train_loss: 0.1954\n",
      "Epoch [217/300], train_loss: 0.1947\n",
      "Epoch [218/300], train_loss: 0.1957\n",
      "Epoch [219/300], train_loss: 0.1953\n",
      "Epoch [220/300], train_loss: 0.1957\n",
      "Epoch [221/300], train_loss: 0.1960\n",
      "Epoch [222/300], train_loss: 0.1963\n",
      "Epoch [223/300], train_loss: 0.1950\n",
      "Epoch [224/300], train_loss: 0.1946\n",
      "Epoch [225/300], train_loss: 0.1950\n",
      "Epoch [226/300], train_loss: 0.1948\n",
      "Epoch [227/300], train_loss: 0.1954\n",
      "Epoch [228/300], train_loss: 0.1947\n",
      "Epoch [229/300], train_loss: 0.1941\n",
      "Epoch [230/300], train_loss: 0.1943\n",
      "Epoch [231/300], train_loss: 0.1942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [232/300], train_loss: 0.1953\n",
      "Epoch [233/300], train_loss: 0.1947\n",
      "Epoch [234/300], train_loss: 0.1942\n",
      "Epoch [235/300], train_loss: 0.1936\n",
      "Epoch [236/300], train_loss: 0.1943\n",
      "Epoch [237/300], train_loss: 0.1940\n",
      "Epoch [238/300], train_loss: 0.1942\n",
      "Epoch [239/300], train_loss: 0.1932\n",
      "Epoch [240/300], train_loss: 0.1941\n",
      "Epoch [241/300], train_loss: 0.1936\n",
      "Epoch [242/300], train_loss: 0.1933\n",
      "Epoch [243/300], train_loss: 0.1936\n",
      "Epoch [244/300], train_loss: 0.1942\n",
      "Epoch [245/300], train_loss: 0.1945\n",
      "Epoch [246/300], train_loss: 0.1946\n",
      "Epoch [247/300], train_loss: 0.1933\n",
      "Epoch [248/300], train_loss: 0.1923\n",
      "Epoch [249/300], train_loss: 0.1933\n",
      "Epoch [250/300], train_loss: 0.1926\n",
      "Epoch [251/300], train_loss: 0.1931\n",
      "Epoch [252/300], train_loss: 0.1933\n",
      "Epoch [253/300], train_loss: 0.1924\n",
      "Epoch [254/300], train_loss: 0.1920\n",
      "Epoch [255/300], train_loss: 0.1924\n",
      "Epoch [256/300], train_loss: 0.1934\n",
      "Epoch [257/300], train_loss: 0.1927\n",
      "Epoch [258/300], train_loss: 0.1925\n",
      "Epoch [259/300], train_loss: 0.1915\n",
      "Epoch [260/300], train_loss: 0.1922\n",
      "Epoch [261/300], train_loss: 0.1921\n",
      "Epoch [262/300], train_loss: 0.1922\n",
      "Epoch [263/300], train_loss: 0.1922\n",
      "Epoch [264/300], train_loss: 0.1919\n",
      "Epoch [265/300], train_loss: 0.1915\n",
      "Epoch [266/300], train_loss: 0.1918\n",
      "Epoch [267/300], train_loss: 0.1919\n",
      "Epoch [268/300], train_loss: 0.1919\n",
      "Epoch [269/300], train_loss: 0.1926\n",
      "Epoch [270/300], train_loss: 0.1914\n",
      "Epoch [271/300], train_loss: 0.1916\n",
      "Epoch [272/300], train_loss: 0.1920\n",
      "Epoch [273/300], train_loss: 0.1910\n",
      "Epoch [274/300], train_loss: 0.1910\n",
      "Epoch [275/300], train_loss: 0.1921\n",
      "Epoch [276/300], train_loss: 0.1915\n",
      "Epoch [277/300], train_loss: 0.1914\n",
      "Epoch [278/300], train_loss: 0.1914\n",
      "Epoch [279/300], train_loss: 0.1914\n",
      "Epoch [280/300], train_loss: 0.1914\n",
      "Epoch [281/300], train_loss: 0.1920\n",
      "Epoch [282/300], train_loss: 0.1904\n",
      "Epoch [283/300], train_loss: 0.1909\n",
      "Epoch [284/300], train_loss: 0.1921\n",
      "Epoch [285/300], train_loss: 0.1913\n",
      "Epoch [286/300], train_loss: 0.1912\n",
      "Epoch [287/300], train_loss: 0.1911\n",
      "Epoch [288/300], train_loss: 0.1911\n",
      "Epoch [289/300], train_loss: 0.1906\n",
      "Epoch [290/300], train_loss: 0.1905\n",
      "Epoch [291/300], train_loss: 0.1900\n",
      "Epoch [292/300], train_loss: 0.1902\n",
      "Epoch [293/300], train_loss: 0.1911\n",
      "Epoch [294/300], train_loss: 0.1899\n",
      "Epoch [295/300], train_loss: 0.1898\n",
      "Epoch [296/300], train_loss: 0.1896\n",
      "Epoch [297/300], train_loss: 0.1899\n",
      "Epoch [298/300], train_loss: 0.1911\n",
      "Epoch [299/300], train_loss: 0.1903\n",
      "Epoch [300/300], train_loss: 0.1896\n",
      "elapsed_time: 235.052s\n"
     ]
    }
   ],
   "source": [
    "# Train Deeplog anomaly detection model\n",
    "import numpy as np\n",
    "import torch\n",
    "from deeplog import train_deeplog, test_deeplog\n",
    "train_data = np.load('./data/train_data.npz')\n",
    "train_normal_seq = train_data['train_normal_seq']\n",
    "train_normal_label = train_data['train_normal_label']\n",
    "model = train_deeplog(train_normal_seq, train_normal_label)\n",
    "torch.save(model, './save/LSTM_onehot.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57838fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time: 3048.796s\n",
      "false positive (FP): 432, false negative (FN): 893, Precision: 88.203%, Recall: 78.341%, F1-measure: 82.980%\n",
      "Finished Predicting\n"
     ]
    }
   ],
   "source": [
    "# Validate the performance of trained model\n",
    "test_normal_loader = np.load('./data/test_normal_loader.npy',allow_pickle=True)\n",
    "test_abnormal_loader = np.load('./data/test_abnormal_loader.npy',allow_pickle=True)\n",
    "test_deeplog(model, test_normal_loader, test_abnormal_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51c9bd",
   "metadata": {},
   "source": [
    "# Interpret your interested anomaly in four steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987a1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Initialize <Univariate Timeseries Interptreter> for Model <LSTM_onehot>\n",
      "\n",
      "Visualize Interpretation (Table View)\n",
      "+------+-------------------------------------+-------+------+-------------------------------------+\n",
      "| Ano. |               Meaning               | Diff. | Ref. |               Meaning*              |\n",
      "+------+-------------------------------------+-------+------+-------------------------------------+\n",
      "|  4   |      Receiving blk* src&dest:*      |       |  4   |      Receiving blk* src&dest:*      |\n",
      "|  10  |  PktResponder* for blk* terminating |       |  10  |  PktResponder* for blk* terminating |\n",
      "|  9   |       PktResponder* Exception       |       |  9   |       PktResponder* Exception       |\n",
      "|  13  |  Exception in receiveBlock for blk* |       |  13  |  Exception in receiveBlock for blk* |\n",
      "|  6   |   writeBlock* received exception*   |       |  6   |   writeBlock* received exception*   |\n",
      "|  7   | PktResponder* for blk* Interrupted. |       |  7   | PktResponder* for blk* Interrupted. |\n",
      "|  10  |  PktResponder* for blk* terminating |       |  10  |  PktResponder* for blk* terminating |\n",
      "|  13  |  Exception in receiveBlock for blk* |       |  13  |  Exception in receiveBlock for blk* |\n",
      "|  6   |   writeBlock* received exception*   |       |  6   |   writeBlock* received exception*   |\n",
      "|  10  |  PktResponder* for blk* terminating |       |  10  |  PktResponder* for blk* terminating |\n",
      "|  9   |       PktResponder* Exception       |   !=  |  1   |        Verification succeeded       |\n",
      "+------+-------------------------------------+-------+------+-------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Step 1: Load your model\"\"\"\n",
    "from deeplog import LSTM_onehot\n",
    "import torch\n",
    "model = torch.load('save/LSTM_onehot.pth.tar')\n",
    "\n",
    "\"\"\"Step 2: Find an anomaly you are interested in\"\"\"\n",
    "import sys\n",
    "sys.path.append('../../deepaid/')\n",
    "from utils import deeplogtools_seqformat\n",
    "abnormal_data = np.load('data/abnormal_data.npy')\n",
    "idx = 100\n",
    "seq, label, anomaly_timeseries = deeplogtools_seqformat(model, abnormal_data, num_candidates=9, index=idx)\n",
    "# print(seq.shape,label.shape)\n",
    "\n",
    "\"\"\"Step 3: Create a DeepAID Interpreter\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"../../deepaid/interpreters/\")\n",
    "from timeseries_onehot import UniTimeseriesAID\n",
    "feature_desc = np.load('data/log_key_meanning.npy') # feature_description\n",
    "my_interpreter = UniTimeseriesAID(model, feature_desc=feature_desc, class_num=28)\n",
    "\n",
    "\"\"\"Step 4: Interpret your anomaly and show the result\"\"\"\n",
    "interpretation = my_interpreter(seq, label)\n",
    "my_interpreter.show_table(anomaly_timeseries, interpretation)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
